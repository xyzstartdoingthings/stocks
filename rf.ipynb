{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from ta.trend import MACD\n",
    "from ta.volatility import BollingerBands\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.momentum import StochasticOscillator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, symbol, start, end):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        \n",
    "        # Fetching the historical prices\n",
    "        df = yf.download(symbol, start, end)\n",
    "        \n",
    "        # Defining the indicators\n",
    "        df['RSI'] = RSIIndicator(df['Close']).rsi()\n",
    "        df['MACD'] = MACD(df['Close']).macd_diff()\n",
    "        df['EMA_5'] = df['Close'].ewm(span=5, adjust=False).mean()\n",
    "        df['EMA_10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
    "        df['EMA_20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
    "        \n",
    "        # Dropping the NaN rows\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # Prices includes all the available indicators\n",
    "        self.prices = df.values\n",
    "        \n",
    "        # Current index in the prices data\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Action space is to either Buy, Sell or Hold\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(1,))\n",
    "        \n",
    "        # Observation space includes the prices and current balance and shares\n",
    "        # which are initialized to be zero\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(9,))\n",
    "        \n",
    "        # Starting balance, you can change this to whatever you want\n",
    "        self.balance = 10000\n",
    "        \n",
    "        # Number of shares bought\n",
    "        self.shares_bought = 0\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Fetch the current price and indicators\n",
    "        price = self.prices[self.current_step, :4].mean()\n",
    "        volume = self.prices[self.current_step, 5]\n",
    "        rsi = self.prices[self.current_step, 6]\n",
    "        macd = self.prices[self.current_step, 7]\n",
    "        ema_5 = self.prices[self.current_step, 8]\n",
    "        ema_10 = self.prices[self.current_step, 9]\n",
    "        ema_20 = self.prices[self.current_step, 10]\n",
    "        action = action[0]\n",
    "        if action < 0:  # sell\n",
    "            if self.shares_bought > 0:\n",
    "                self.balance = self.balance + self.shares_bought * (-action) * price\n",
    "                self.shares_bought = self.shares_bought*(1+action)\n",
    "\n",
    "        elif action > 0:  # buy\n",
    "            if self.balance > 0:\n",
    "                self.shares_bought = self.shares_bought + self.balance*action/price\n",
    "                self.balance = self.balance - self.balance*action\n",
    "\n",
    "        # Next day\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= len(self.prices) - 1:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        # Calculate net worth and normalize it by the initial balance\n",
    "        net_worth = self.balance + self.shares_bought * price\n",
    "\n",
    "        reward = net_worth - 10000  # The reward is the increase in net worth\n",
    "\n",
    "        # The state includes the indicators and current balance and shares\n",
    "        obs = np.array([price, volume, rsi, macd, ema_5, ema_10, ema_20, self.balance, self.shares_bought])\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset to day 1\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Reset balance\n",
    "        self.balance = 10000\n",
    "        self.shares_bought = 0\n",
    "\n",
    "        # Fetch first day prices and indicators\n",
    "        price = self.prices[self.current_step, :4].mean()\n",
    "        volume = self.prices[self.current_step, 5]\n",
    "        rsi = self.prices[self.current_step, 6]\n",
    "        macd = self.prices[self.current_step, 7]\n",
    "        ema_5 = self.prices[self.current_step, 8]\n",
    "        ema_10 = self.prices[self.current_step, 9]\n",
    "        ema_20 = self.prices[self.current_step, 10]\n",
    "\n",
    "        # The state includes the indicators and current balance and shares\n",
    "        obs = np.array([price, volume, rsi, macd, ema_5,ema_10, ema_20, self.balance, self.shares_bought])\n",
    "\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fuway\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 2.36e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 2.36e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 471       |\n",
      "|    mean_reward     | 2.36e+06  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1500      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.74e+05 |\n",
      "|    critic_loss     | 8.88e+11  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1224      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 2.36e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 471       |\n",
      "|    mean_reward     | 2.36e+06  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 2500      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.96e+05 |\n",
      "|    critic_loss     | 7.09e+10  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2448      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 2.36e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 2.36e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 471       |\n",
      "|    mean_reward     | 2.36e+06  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 4000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.23e+05 |\n",
      "|    critic_loss     | 4.03e+10  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3672      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 2.36e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.22e+03 |\n",
      "|    ep_rew_mean     | 2.54e+06 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 227      |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 4896     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 2.36e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.5e+05 |\n",
      "|    critic_loss     | 2.81e+10 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4896     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 2.36e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 2.36e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 471       |\n",
      "|    mean_reward     | 2.36e+06  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 6500      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.04e+05 |\n",
      "|    critic_loss     | 1.09e+10  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 6120      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 2.36e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 471       |\n",
      "|    mean_reward     | 2.36e+06  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 7500      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.32e+05 |\n",
      "|    critic_loss     | 5e+09     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 7344      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 2.36e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 2.36e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 471       |\n",
      "|    mean_reward     | 2.36e+06  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 9000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.99e+04 |\n",
      "|    critic_loss     | 2.47e+09  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 8568      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=2363909.16 +/- 0.00\n",
      "Episode length: 471.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 2.36e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.22e+03 |\n",
      "|    ep_rew_mean     | 2.52e+06 |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 210      |\n",
      "|    time_elapsed    | 46       |\n",
      "|    total_timesteps | 9792     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 24\u001b[0m\n\u001b[0;32m     19\u001b[0m eval_callback \u001b[39m=\u001b[39m EvalCallback(eval_env, best_model_save_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./logs/\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m                              log_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./logs/\u001b[39m\u001b[39m'\u001b[39m, eval_freq\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m,\n\u001b[0;32m     21\u001b[0m                              deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, render\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[39m# Train agent\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m, callback\u001b[39m=\u001b[39;49meval_callback)\n",
      "File \u001b[1;32mc:\\Users\\fuway\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\ddpg\\ddpg.py:123\u001b[0m, in \u001b[0;36mDDPG.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m: SelfDDPG,\n\u001b[0;32m    116\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    122\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfDDPG:\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    124\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    125\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    126\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    127\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    128\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    129\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    130\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\fuway\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\td3\\td3.py:216\u001b[0m, in \u001b[0;36mTD3.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    208\u001b[0m     \u001b[39mself\u001b[39m: SelfTD3,\n\u001b[0;32m    209\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfTD3:\n\u001b[1;32m--> 216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    217\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    218\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    219\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    220\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    221\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    222\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\fuway\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:330\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[39m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    329\u001b[0m         \u001b[39mif\u001b[39;00m gradient_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 330\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, gradient_steps\u001b[39m=\u001b[39;49mgradient_steps)\n\u001b[0;32m    332\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[0;32m    334\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\fuway\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\td3\\td3.py:160\u001b[0m, in \u001b[0;36mTD3.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_updates \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    159\u001b[0m \u001b[39m# Sample replay buffer\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m replay_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer\u001b[39m.\u001b[39;49msample(batch_size, env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vec_normalize_env)\n\u001b[0;32m    162\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m    163\u001b[0m     \u001b[39m# Select action according to policy and add clipped noise\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     noise \u001b[39m=\u001b[39m replay_data\u001b[39m.\u001b[39mactions\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnormal_(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_policy_noise)\n",
      "File \u001b[1;32mc:\\Users\\fuway\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:285\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[1;34m(self, batch_size, env)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[39mSample elements from the replay buffer.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[39mCustom sampling when using memory efficient variant,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[39m:return:\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimize_memory_usage:\n\u001b[1;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msample(batch_size\u001b[39m=\u001b[39;49mbatch_size, env\u001b[39m=\u001b[39;49menv)\n\u001b[0;32m    286\u001b[0m \u001b[39m# Do not sample the element with index `self.pos` as the transitions is invalid\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39m# (we use only one array to store `obs` and `next_obs`)\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull:\n",
      "File \u001b[1;32mc:\\Users\\fuway\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:111\u001b[0m, in \u001b[0;36mBaseBuffer.sample\u001b[1;34m(self, batch_size, env)\u001b[0m\n\u001b[0;32m    109\u001b[0m upper_bound \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos\n\u001b[0;32m    110\u001b[0m batch_inds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, upper_bound, size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m--> 111\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_samples(batch_inds, env\u001b[39m=\u001b[39;49menv)\n",
      "File \u001b[1;32mc:\\Users\\fuway\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:312\u001b[0m, in \u001b[0;36mReplayBuffer._get_samples\u001b[1;34m(self, batch_inds, env)\u001b[0m\n\u001b[0;32m    301\u001b[0m     next_obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_normalize_obs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_observations[batch_inds, env_indices, :], env)\n\u001b[0;32m    303\u001b[0m data \u001b[39m=\u001b[39m (\n\u001b[0;32m    304\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_normalize_obs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations[batch_inds, env_indices, :], env),\n\u001b[0;32m    305\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions[batch_inds, env_indices, :],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_normalize_reward(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards[batch_inds, env_indices]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), env),\n\u001b[0;32m    311\u001b[0m )\n\u001b[1;32m--> 312\u001b[0m \u001b[39mreturn\u001b[39;00m ReplayBufferSamples(\u001b[39m*\u001b[39m\u001b[39mtuple\u001b[39;49m(\u001b[39mmap\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_torch, data)))\n",
      "File \u001b[1;32mc:\\Users\\fuway\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:135\u001b[0m, in \u001b[0;36mBaseBuffer.to_torch\u001b[1;34m(self, array, copy)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[39mConvert a numpy array to a PyTorch tensor.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[39mNote: it copies the data by default\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39m:return:\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m th\u001b[39m.\u001b[39;49mtensor(array, device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    136\u001b[0m \u001b[39mreturn\u001b[39;00m th\u001b[39m.\u001b[39mas_tensor(array, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# Create environment\n",
    "env = TradingEnv('AAPL', '2015-01-01', '2020-01-01')\n",
    "\n",
    "# Create action noise (required for DDPG)\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "# Initialize agent\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, verbose=1, device='cuda')\n",
    "\n",
    "# Create evaluation environment\n",
    "eval_env = TradingEnv('AAPL', '2020-01-01', '2022-01-01')\n",
    "\n",
    "# Create evaluation callback\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=500,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "# Train agent\n",
    "model.learn(total_timesteps=10000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Total rewards: -206059.67392272642\n"
     ]
    }
   ],
   "source": [
    "# Create a new environment with the test data\n",
    "test_env = TradingEnv('AAPL', '2022-01-01', '2023-01-01')\n",
    "\n",
    "# Initialize the state using the first observation\n",
    "obs = test_env.reset()\n",
    "\n",
    "# To keep track of rewards\n",
    "total_rewards = 0\n",
    "\n",
    "# Run until the environment is done\n",
    "while True:\n",
    "    # Get the action from the agent\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "    # Take a step in the environment\n",
    "    obs, reward, done, info = test_env.step(action)\n",
    "\n",
    "    # Update the total reward\n",
    "    total_rewards += reward\n",
    "\n",
    "    # If the environment is done, break the loop\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f'Total rewards: {total_rewards}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "type numpy.ndarray doesn't define __round__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fuway\\Desktop\\stocks\\rf.ipynb Cell 5\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fuway/Desktop/stocks/rf.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m ema_10 \u001b[39m=\u001b[39m prices[\u001b[39m0\u001b[39m, \u001b[39m9\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fuway/Desktop/stocks/rf.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m ema_20 \u001b[39m=\u001b[39m prices[\u001b[39m0\u001b[39m, \u001b[39m10\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fuway/Desktop/stocks/rf.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mround\u001b[39;49m(np\u001b[39m.\u001b[39;49marray([price, volume, rsi, macd, ema_5,ema_10, ema_20, \u001b[39m10000\u001b[39;49m, \u001b[39m0\u001b[39;49m]))\n",
      "\u001b[1;31mTypeError\u001b[0m: type numpy.ndarray doesn't define __round__ method"
     ]
    }
   ],
   "source": [
    "df = yf.download('AAPL', '2020-01-01', '2021-01-01')\n",
    "df['RSI'] = RSIIndicator(df['Close']).rsi()\n",
    "df['MACD'] = MACD(df['Close']).macd_diff()\n",
    "df['EMA_5'] = df['Close'].ewm(span=5, adjust=False).mean()\n",
    "df['EMA_10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
    "df['EMA_20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
    "prices = df.dropna().values\n",
    "price = prices[0, :4].mean()\n",
    "volume = prices[0, 5]\n",
    "rsi = prices[0, 6]\n",
    "macd = prices[0, 7]\n",
    "ema_5 = prices[0, 8]\n",
    "ema_10 = prices[0, 9]\n",
    "ema_20 = prices[0, 10]\n",
    "round(np.array([price, volume, rsi, macd, ema_5,ema_10, ema_20, 10000, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fuway\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  sample[upp_bounded] = (\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(600, 800, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_shape = (600, 800, 3)\n",
    "observation_space = spaces.Box(low = np.zeros(observation_shape), \n",
    "                                    high = np.ones(observation_shape),\n",
    "                                    dtype = np.float16)\n",
    "(np.ones(observation_shape) * 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 800, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "use_cuda = torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
